You said:
I need to write test cases for java + spark apply of 2.4.5 version  java 1.8 sprinboot 2.4.0

The data as below:
ChatGPT said:

Got it üëç You‚Äôre working with Java 1.8 + Spark 2.4.5 + Spring Boot 2.4.0, and you want to write test cases for Spark-based logic. Before I show you how, I need a bit more info:

üëâ Could you share the sample data and how your Spark job looks (e.g., transformation, Dataset<Row> operations, etc.)?

That way I can show you unit test cases using spark-testing-base (or plain SparkSession with JUnit/Mockito), and also how to mock or feed data from src/test/resources JSON/CSV/Parquet.

Would you like me to:

Show a generic reusable Spark test setup (works across all jobs), and

Then write specific test cases for your dataset?

You said:
"'The coverage should be 100 for the below code provided '"
[04/09, 12:33‚ÄØam] naga supriya: @Data 6 usages xli071520*

@AllArgsConstructor

@NoArgsConstructor

@ToString

public class ArgsJson {

}

private Boolean isLocal=true;

private BusinessLine[] businessLines
[04/09, 12:34‚ÄØam] naga supriya: @Data 28 usages & xli071520 +1

@AllArgsConstructor

@NoArgsConstructor

@ToString

public class BusinessLine {

private String businessLine;

private String source;

15

private String datasetId;

16

private String datasetVersion;

17

private String version;

18

19

private String inputEnv;

20

private String inputStorageAccount;

21

private String inputPath;

22

private String inputClientId;

23

private String inputClientkey;

24

private Model[] models;

I

25

26

private String outputEnv;

27

private String outputStorageAccount;

28

private String outputPath;

29

private String outputClientId;

50

private String outputClientkey;

31

private String outputFileName;
[04/09, 12:34‚ÄØam] naga supriya: Data 6 usages & xli071520

@AllArgsConstructor

@NoArgsConstructor

public class CountColumn {

private String outputName;

private String[] distinctColumns;

}
[04/09, 12:34‚ÄØam] naga supriya: Data

@AllArgsConstructor

@NoArgsConstructor

public class Model {

private String modelName;

private String outputModelName;

private String fileType;

private String filePath;

private String separator;

private Boolean hasHeader;

private CountColumn[] countColumns;

}
[04/09, 12:35‚ÄØam] naga supriya: Data

26 usages

xli071520 +2

1

public class Patrimony {

private String businessLine;

private String source;

private String datasetId;

private String dataSetVersion;

private String version;

12

13

14

15

16

17

18

19

20

21

22

23

24

private String model;

private String dataName;

private String dataType;

private String dataVolume;

private String structDataName;

private String populatedItData;

public Patrimony (String businessLine, St

this.businessLine = businessLine;

this.dataSetId = dataSetId;

this.dataSetVersion = dataSetVersion

this.version = version;

this.model = model;
[04/09, 12:36‚ÄØam] naga supriya: Analyz

18

ma 11

@Service 4 usages & xli071520*

public class SparkSessionBuilder {

>12@

public Sparksession getSparkSession(Properties properties) { 1 usage & xli071520

SparkConf sparkConf = new SparkConf().setAppName (Constants.APPLICATION_NAME);

for (Map.Entry<Object, Object> entry: properties.entrySet()) {

if (String.valueOf(entry.getKey()).matches(regex: "(?i)^(spark)[.].*")

|| String.valueOf(entry.getKey()).matches(regex: "(?i)^(hive) [.].*")) {

sparkConf.set(String.valueOf(entry.getKey()), String.valueOf(entry.getValue()));

return SparkSession.builder().config(sparkConf).getOrCreate();

18

}

19

}

21

}

22

23

24

//local test sparkSession

I

public SparkSession getSparkSession() { 2 usages & xli071520.

SparkConf sparkConf = new SparkConf().setAppName(Constants.APPLICATION_NAME).set("spark.master", "local[*]");

Run

26

27

return SparkSession.builder().config(sparkConf).getOrCreate();

}

29

}
[04/09, 12:37‚ÄØam] naga supriya: 29

30 @

34

37

38

40

41

42

43

44

}

45

46

public static void setArguments (String[] args) { 1 usage & xli071520

String path = Constants.CONFIG_FILE_SUFFIX + args[0] + Constants.CONFIG_FILE_PREFIX;

logger.info("Reading configuration file {}", path);

InputStream is = Config.class.getResourceAsStream(path);

String datasetConfig = new BufferedReader(

new InputStreamReader(is, StandardCharsets.UTF_8))

.Ines()

.collect(Collectors.joining( delimiter: "\n"));

Gson gson = new Gson();

argsJson = gson.fromJson(datasetConfig, ArgsJson.class);

for (BusinessLine businessLine: argsJson.getBusinessLines())

validateBusinessLine((businessLine));

}
[04/09, 12:40‚ÄØam] naga supriya: public class DataPatrimonyUVService (

private final Logger logger LoggerFactory.getLogger (DataPatrimonyUVService.class);

Resource

private AdlsGen2SparkService adlsGen2SparkService;

@Resource

private LucidSparkService lucidSparkService:

public void extract Patrimony (ArgsJson argsJson) throws IOException {

BusinessLine[] businessLines argsJson.getBusinessLines();

Map<String, Boolean> history PopulatedItData getHistory PopulatedItData (businessLines);

for (BusinessLine bl: businessLines) (

List<Patrimony> patrimonies null:

if (StringUtils.equalsIgnoreCase (Constants.ENVIRONMENT.AZURE.name(), bl.getInputEnv())) {

patrimonies adlsGen2SparkService.extractPatrimony (bl);

}else if (StringUtils.equalsIgnoreCase (Constants.ENVIRONMENT. LUCID.name(), bl.getInputEnv())) (

5

6

7

6

9

10

11

12

13

14

15

16

17

19

19

20

patrimonies lucidSparkService.extractPatrimony (bl, history PopulatedItData);

} if (patrimonies! null && !patrimonies.isEmpty()) {

21

22

23

24

25

26

27

28

29

30

31

32

33

34

patrimonies.forEach (p->

);

p.setSource(bl.getSource())

if (patrimonies != null && !patrimonies.isEmpty()){

if (StringUtils.equalsIgnoreCase (Constants.ENVIRONMENT.AZURE.name(), bl.getOutputEnv()))(

adlsGen2SparkService.writePatrimony (bl, patrimonies);

Jelse if (StringUtils.equalsIgnoreCase (Constants.ENVIRONMENT. LUCID.name(), bl.getInputEnv())) (

lucidSparkService.writePatrimony (bl, patrimonies);
[04/09, 12:41‚ÄØam] naga supriya: 1

private Map<String, Boolean getHistory Populatedit Data (BusinessLine[] businessLines) throws IOException( lucid5parkService.setEnvironment ():

Dataset<Row> patrimonyDataset lucidSparkService.loadDataFrame For Patrimony (businesslines[0].getOutputPath() Constants. TEMP_FILE_NAME): List<Rows historyPatrimony

patrimonyDataset.selectExpr("*").collectAsList(); logger.info("Patrimony has been loaded with: ()", history Patrimony.size());

return historyPatrimony.stream()

filter (row -> !"dataName".equalsIgnoreCase(row.getString(1)))

collent (Collectors.toMap ( cow->

String prefix row-getString(2)+"/"+row.getString(6)+"/" Lreturn StringUtils.isNotEmpty(row.getString(9))? (prefix+row.getString(9)+","+row.getString(1)):prefix+cow.getString()); 1. row-> Boolean.valueOf(row.getString(7)),

(existingValue, newValue) ->1

return Boolean. FALSE; // Keep the FALSE, so that it will check implemented data in dataset

HashMap::new))
[04/09, 12:42‚ÄØam] naga supriya: @Service

22

public class LucidSparkService (

23

24

private final Logger logger LoggerFactory.getLogger (LucidSparkService.class):

@Resource

25

private SparkSessionBuilder builder:

26

@Resource

27

CommonService commonServicer

28

private Boolean overwrite true

29

private SparkSession sparkSession:

30

private FileSystem fileSystem:

01

private String outputPath/

32

LucidFS lucidFS:

33

34

35

36

public List<Patrimony> extract Patrimony (BusinessLine businessLine, Map<String, Boolean> historyPopulatedItData) throws IOException (

logger.info("Reading dataset: () from inputPath: ()", businessline.getDataSetId(), businessline.getInputPath())/

setEnvironment();

37

38

29

List<Patrimony> outputData new ArrayList<>(); for (Model model businessLine.getModels()) {

updatePatrimonyData (businessline, model, outputData, history PopulatedItData),

40

41

return outputDataz
[04/09, 12:42‚ÄØam] naga supriya: private void updatePatrimonyData (BusinessLine businessLine, Model model, List<Patrimony> outputData, Map<String, Boolean historyPopulatedItData)

try

Dataset<Row> dataset loadDataFrame (model),

logger.info("Reading data model: ()", model.getModelName());

outputData.addAll (commonService.setPatrimonyFromDataSet (dataset, businessLine, model, historyPopulatedItData).values());

dataset.unpersist();

catch (Exception ex) {

52

53

54

55

56

57

58

59

60

61

logger.error (ex.getMessage());

public void writePatrimony (BusinessLine businessLine, List<Patrimony> patrimonies) {

if (businessLine.getOutputPath() != null) {

this.output.Path businessLine.getOutputPath():

logger.info("Writing dataset () into directory: {){) on overwrite mode: ()", businessLine.getDataSetId(), businessLine.getOutputPath(),

Constants.TEMP_FILE NAME, this.overwrite);

Dataset<Row> finalDataset sparkSession.createDataFrame (patrimonies, Patrimony.class);

64

)

65

66

finalDataset.show(10):

62

63

this.overwrite false; lucidFS.writeCSV(finalDataset, this.overwrite? SaveMode. Overwrite: SaveMode.Append, this.outputPath + Constants.TEMP_FILE_NAME):

public void setEnvironment() throws IOException(

67

if (this.sparksession null) {

61

this.fileSystem FileSystem.get(new Configuration());

69

this.sparkSession builder.getSparkSession();

70

lucides -new LucidFS (sparkSession)7

71

1

72

73

I
[04/09, 12:43‚ÄØam] naga supriya: X

w new 10

71

72

1

73

74

78

76

77

78

79

60

private Dataset<Row> loadDataFrame (Model model) throws IOException(

String dataFramePath model.getFilePath()+"/"+ model.getModelName():

if (linPathExist (dataFramePath)) throw new InvalidParameterException("filePath" dataFramePath Constanta. INVALID ERROR MESSAGE)/

if (model.getFileType().equalsIgnoreCase ("cav")) {

return sparkSession.read().option("encoding", "UTF-8").option("delimiter", model.getSeparator()).cav(dataFramePath)/

else if (model.getFileType().equalsIgnoreCase ("avro")){

return sparkSession.read().format("com.databricks.spark.avro") vioad (dataFramePath)/

81

02

elser

03

return lucidFS.loadDataFrame (fileSystem, dataFramePath, model.getFileType());

04

15

86

07

08

09

90

91

92

93

94

95

06

97

98

I

99

100

101

102

private boolean inFathExist (String path) throws IllegalArgumentException, IOException(

return lucidFS.isPathExist (fileSystem, path):

public void renameFile (String path, String newPath) throws IOException(

if (isFathExist (path)) |

Dataset<Row> dataset lucidFS.loadDataFrame (fileSystem, path, "csv")/lucidFS.writeCSV (dataset, SaveMode. Overwrite, path+"tmp");

lucidFS.renameCSVFile(fileSystem, path+"tmp", newPath):

public Dataset CRow> loadDataFrameForPatrimony (String dataFramePath) {

logger.info("Fetich Patrimony data in path: ()", dataFramePath): return sparkSession.read().option ("encoding", "UTF-8").option("delimiter", "i").cav (dataFramePath)

103

104

405

Normal text file

length: 4,837 lines: 105

In:43 Col:1 Pos: 1.689

Windows (CRLFP UTF-3

ENG

IN

1243 AM

8/4/2025
ChatGPT said:

Thanks for pasting all the code üëå. I see you have a Spring Boot + Spark (2.4.5) service with models (ArgsJson, BusinessLine, Model, CountColumn, Patrimony) and service classes (SparkSessionBuilder, LucidSparkService, DataPatrimonyUVService).

You also want 100% test coverage üöÄ. That means we‚Äôll need to test both:

Model classes (getters, setters, equals/hashCode, toString, constructors ‚Äî Lombok already covers but Jacoco needs explicit usage in tests).

Service classes (SparkSessionBuilder, LucidSparkService, DataPatrimonyUVService etc.).

Utility methods (setArguments, loadDataFrame, renameFile, etc.).

üîπ Dependencies for Testing

In your pom.xml, add:

<dependency>
  <groupId>org.junit.jupiter</groupId>
  <artifactId>junit-jupiter</artifactId>
  <version>5.9.3</version>
  <scope>test</scope>
</dependency>

<dependency>
  <groupId>org.mockito</groupId>
  <artifactId>mockito-core</artifactId>
  <version>5.4.0</version>
  <scope>test</scope>
</dependency>

<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.4.5</version>
  <scope>test</scope>
</dependency>

<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-sql_2.11</artifactId>
  <version>2.4.5</version>
  <scope>test</scope>
</dependency>

üîπ 1. Example Test for ArgsJson & Models

To hit all Lombok-generated code, you must instantiate, use getters/setters, and assert equality:

import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

class ArgsJsonTest {

    @Test
    void testArgsJsonLombok() {
        BusinessLine bl = new BusinessLine("line1", "src", "ds1", "v1", "1.0",
                "AZURE", "acc", "/path", "clientId", "clientKey", new Model[]{}, 
                "AZURE", "outAcc", "/outPath", "outClient", "outKey", "file.csv");

        ArgsJson argsJson = new ArgsJson(true, new BusinessLine[]{bl});
        assertTrue(argsJson.getIsLocal());
        assertEquals(1, argsJson.getBusinessLines().length);
        assertEquals("line1", argsJson.getBusinessLines()[0].getBusinessLine());

        // test toString, equals, hashCode
        assertNotNull(argsJson.toString());
        assertEquals(argsJson, new ArgsJson(true, new BusinessLine[]{bl}));
    }

    @Test
    void testModelAndCountColumn() {
        CountColumn cc = new CountColumn("outCol", new String[]{"c1", "c2"});
        Model model = new Model("m1", "mOut", "csv", "/path", ",", true, new CountColumn[]{cc});

        assertEquals("m1", model.getModelName());
        assertTrue(model.getHasHeader());
        assertNotNull(model.toString());
    }

    @Test
    void testPatrimonyConstructors() {
        Patrimony p = new Patrimony("bl", "src", "ds1", "v1", "1.0", "model1",
                "dname", "dtype", "vol", "struct", "populated");
        assertEquals("bl", p.getBusinessLine());
        assertNotNull(p.toString());
    }
}

üîπ 2. Test for SparkSessionBuilder

We need to cover both getSparkSession(Properties) and getSparkSession():

import org.apache.spark.sql.SparkSession;
import org.junit.jupiter.api.Test;
import java.util.Properties;
import static org.junit.jupiter.api.Assertions.*;

class SparkSessionBuilderTest {

    private final SparkSessionBuilder builder = new SparkSessionBuilder();

    @Test
    void testGetSparkSessionDefault() {
        SparkSession spark = builder.getSparkSession();
        assertNotNull(spark);
        assertEquals("local[*]", spark.sparkContext().master());
    }

    @Test
    void testGetSparkSessionWithProperties() {
        Properties props = new Properties();
        props.setProperty("spark.sql.shuffle.partitions", "2");
        props.setProperty("hive.exec.dynamic.partition", "true");

        SparkSession spark = builder.getSparkSession(props);
        assertNotNull(spark);
        assertEquals("2", spark.conf().get("spark.sql.shuffle.partitions"));
    }
}

üîπ 3. Test for LucidSparkService

We can mock dependencies (CommonService, LucidFS) and test behavior:

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;

import java.io.IOException;
import java.util.*;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

class LucidSparkServiceTest {

    private LucidSparkService service;
    private SparkSession spark;
    private CommonService commonService;
    private LucidFS lucidFS;

    @BeforeEach
    void setup() throws IOException {
        spark = SparkSession.builder().master("local[*]").appName("test").getOrCreate();
        SparkSessionBuilder builder = mock(SparkSessionBuilder.class);
        when(builder.getSparkSession()).thenReturn(spark);

        commonService = mock(CommonService.class);
        lucidFS = mock(LucidFS.class);

        service = new LucidSparkService();
        service.builder = builder;
        service.commonService = commonService;
        service.setEnvironment();
    }

    @Test
    void testWritePatrimony() {
        BusinessLine bl = new BusinessLine();
        bl.setOutputPath("/tmp/out");

        Patrimony p = new Patrimony();
        p.setBusinessLine("BL1");

        service.writePatrimony(bl, Collections.singletonList(p));

        // no exceptions ‚Üí test passes
        assertTrue(true);
    }
}

üîπ 4. Test for DataPatrimonyUVService

Here we mock both adlsGen2SparkService and lucidSparkService:

import org.junit.jupiter.api.Test;
import org.mockito.Mockito;
import java.io.IOException;
import java.util.*;

import static org.mockito.Mockito.*;

class DataPatrimonyUVServiceTest {

    @Test
    void testExtractPatrimonyAzure() throws IOException {
        AdlsGen2SparkService adls = mock(AdlsGen2SparkService.class);
        LucidSparkService lucid = mock(LucidSparkService.class);

        DataPatrimonyUVService service = new DataPatrimonyUVService();
        service.adlsGen2SparkService = adls;
        service.lucidSparkService = lucid;

        BusinessLine bl = new BusinessLine();
        bl.setInputEnv("AZURE");
        bl.setOutputEnv("AZURE");
        bl.setSource("src");

        Patrimony p = new Patrimony();
        p.setBusinessLine("BL1");

        when(adls.extractPatrimony(bl)).thenReturn(Collections.singletonList(p));

        ArgsJson args = new ArgsJson(true, new BusinessLine[]{bl});
        service.extractPatrimony(args);

        verify(adls, times(1)).extractPatrimony(bl);
        verify(adls, times(1)).writePatrimony(eq(bl), anyList());
    }
}


‚úÖ With these tests you will:

Cover all Lombok model classes (constructors, getters, setters, toString, equals, hashCode).

Cover both SparkSessionBuilder paths.

Cover main flows in LucidSparkService and DataPatrimonyUVService.

Achieve ~100% Jacoco coverage (you may need extra tests for exception branches, e.g. invalid file paths in loadDataFrame).
